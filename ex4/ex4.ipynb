{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04CTldUMqRZS"
      },
      "source": [
        "# IMPORT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pLp1a5wz6kN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxnF5eI-pOFK",
        "outputId": "4f2ad9a4-6b5d-4c5e-a393-08bd9d160b1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4dJ5iGsur6F"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDknvCFtu1lL"
      },
      "outputs": [],
      "source": [
        "pip install mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoRysx0Xu3MH"
      },
      "outputs": [],
      "source": [
        "pip install legacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gV0lG_wMousq"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import PIL.Image\n",
        "from time import perf_counter\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_mlOrLEHu9P2"
      },
      "outputs": [],
      "source": [
        "sys.path.insert(0, '/content/stylegan2-ada-pytorch')\n",
        "import dnnlib\n",
        "sys.path.insert(1, '/content/stylegan2-ada-pytorch')\n",
        "import torch_utils.persistence\n",
        "sys.path.insert(1, '/content/stylegan2-ada-pytorch')\n",
        "import style_mixing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TTQ9uiy6oznI"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgyzFj0mt7o_"
      },
      "source": [
        "# Load GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ummPQEg8voxs"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sys.stdout = open(os.devnull, 'w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M3yHHq0ut_6f"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/nni_ex4/wikiart256-fid689.pkl', 'rb') as f:\n",
        "    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module\n",
        "z = torch.randn([1, G.z_dim]).cuda()    # latent codes\n",
        "c = None                                # class labels (not used in this example)\n",
        "img = G(z, c)                           # NCHW, float32, dynamic range [-1, +1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps-E1nrmqbLl"
      },
      "source": [
        "# Loss Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arfg7_aHqcT5"
      },
      "source": [
        "# Content Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KBwRAfDzqJwR"
      },
      "outputs": [],
      "source": [
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target,):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input, self.target)\n",
        "        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvZaqeaEqj7R"
      },
      "source": [
        "# Style Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FotXZ8agqhtY"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()\n",
        "    features = input.view(a * b, c * d) \n",
        "    G = torch.mm(features, features.t())\n",
        "    return G.div(a * b * c * d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8S-PJ-DDqnZD"
      },
      "outputs": [],
      "source": [
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = gram_matrix(target_feature).detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = gram_matrix(input)\n",
        "        self.loss = F.mse_loss(G, self.target)\n",
        "        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Style Mixing"
      ],
      "metadata": {
        "id": "w2ri6CQDeEyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def style_mixing(G,w_content,w_style,styles_lvl):\n",
        "    w = w_content.clone()\n",
        "    w[styles_lvl] = w_style[styles_lvl]\n",
        "    image = G.synthesis(w[np.newaxis], noise_mode='const')\n",
        "    image = (image.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "    return image[0].cpu().numpy()"
      ],
      "metadata": {
        "id": "gUMFyJLd6uED"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_seeds = [31, 15, 83]\n",
        "style_seeds = [47, 139, 89]\n",
        "content_z = np.stack([np.random.RandomState(seed).randn(G.z_dim) for seed in content_seeds])\n",
        "content_w = G.mapping(torch.from_numpy(content_z).to(device), None)\n",
        "content_images = G.synthesis(content_w, noise_mode=\"const\")\n",
        "content_images=(content_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
        "style_z = np.stack([np.random.RandomState(seed).randn(G.z_dim) for seed in style_seeds])\n",
        "style_w = G.mapping(torch.from_numpy(style_z).to(device), None)\n",
        "style_images = G.synthesis(style_w, noise_mode=\"const\")\n",
        "style_images=(style_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
        "for i in range(len(content_seeds)):\n",
        "    content=content_images[i]\n",
        "    style=style_images[i]\n",
        "    plt.imshow(content)\n",
        "    plt.title(\"content\")\n",
        "    plt.show()\n",
        "    plt.imshow(style)\n",
        "    plt.title(\"style\")\n",
        "    plt.show()\n",
        "    mix=style_mixing(G,content_w[i],style_w[i],range(7,14))\n",
        "    plt.imshow(mix)\n",
        "    plt.title(\"mix\")\n",
        "    plt.show()\n",
        "  "
      ],
      "metadata": {
        "id": "DWOccmgR7FWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(all_seeds)):\n",
        "  PIL.Image.fromarray(all_images[i], 'RGB').save(f\"{all_seeds[i]}.png\")"
      ],
      "metadata": {
        "id": "gPlQ6GatCRMr"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9lZjVVQqrPf"
      },
      "source": [
        "# Importing the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "a9ul9aIWqpdm"
      },
      "outputs": [],
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sJpgh5mqtsd"
      },
      "outputs": [],
      "source": [
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "# create a module to normalize input image so we can easily put it in a\n",
        "# nn.Sequential\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
        "        # directly work with image Tensor of shape [B x C x H x W].\n",
        "        # B is batch size. C is number of channels. H is height and W is width.\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # normalize img\n",
        "        return (img - self.mean) / self.std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X-C3rkzqwf1"
      },
      "outputs": [],
      "source": [
        "content_layers_default = ['conv_4']\n",
        "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
        "                               style_img, content_img,\n",
        "                               content_layers=content_layers_default,\n",
        "                               style_layers=style_layers_default):\n",
        "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
        "\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    model = nn.Sequential(normalization)\n",
        "\n",
        "    i = 0  \n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += 1\n",
        "            name = 'conv_{}'.format(i)\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = 'relu_{}'.format(i)\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            name = 'pool_{}'.format(i)\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            name = 'bn_{}'.format(i)\n",
        "        else:\n",
        "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "            break\n",
        "\n",
        "    model = model[:(i + 1)]\n",
        "\n",
        "    return model, style_losses, content_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0saduHS2q-U4"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Optimizing on W+\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "1INVn1eCq6_B"
      },
      "outputs": [],
      "source": [
        "def get_input_optimizer(input_img):\n",
        "    # this line to show that input is a parameter that requires a gradient\n",
        "    optimizer = optim.Adam([input_img])\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Wjm5tdXnrBZI"
      },
      "outputs": [],
      "source": [
        "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
        "                       content_img, style_img, input_img,content_seed,a, num_steps=300,\n",
        "                       style_weight=1000000, content_weight=1):\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
        "        normalization_mean, normalization_std, style_img, content_img)\n",
        "    input_img.requires_grad_(True)\n",
        "    model.requires_grad_(False)\n",
        "    all_seeds=[content_seed]\n",
        "    all_z = np.stack([np.random.RandomState(seed).randn(G.z_dim) for seed in all_seeds])\n",
        "    all_w = G.mapping(torch.from_numpy(all_z).to(device), None)\n",
        "    input_img = G.synthesis(all_w, noise_mode=\"const\")\n",
        "    all_w = torch.tensor(all_w, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
        "    optimizer = get_input_optimizer(all_w)\n",
        "    run = [0]\n",
        "    for i in range(num_steps):\n",
        "        input_img = (G.synthesis(all_w, noise_mode=\"const\")+1)/2\n",
        "        def closure():\n",
        "            with torch.no_grad():\n",
        "                input_img.clamp_(0, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model(input_img)\n",
        "            style_score = 0\n",
        "            content_score = 0\n",
        "\n",
        "            for sl in style_losses:\n",
        "                style_score += sl.loss\n",
        "            for cl in content_losses:\n",
        "                content_score += cl.loss\n",
        "\n",
        "            style_score *= style_weight\n",
        "            content_score *= content_weight\n",
        "\n",
        "            loss = a * style_score + content_score\n",
        "            loss.backward(retain_graph=True)\n",
        "            return   a * style_score + content_score\n",
        "\n",
        "        optimizer.step(closure)\n",
        "    with torch.no_grad():\n",
        "        input_img.clamp_(0, 1)\n",
        "\n",
        "    return input_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSmIESM7qUXN"
      },
      "source": [
        "# Loading the Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "TGEGYrono2WK"
      },
      "outputs": [],
      "source": [
        "imsize = 256 \n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize(imsize),  \n",
        "    transforms.ToTensor()])  \n",
        "\n",
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "hWbFqa5mo5N-"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.ion()\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.cpu().clone() \n",
        "    image = image.squeeze(0)      \n",
        "    image = transforms.ToPILImage()(image)\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPOmR6iSrUfu"
      },
      "source": [
        "# **RUN Optimization W+**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOU6PmIarHlp"
      },
      "outputs": [],
      "source": [
        "content_seeds = [31, 15, 83]\n",
        "style_seeds = [47, 139, 89]\n",
        "a=[0.1,0.05,0.1]\n",
        "for i in range(len(content_seeds)):\n",
        "  style_img = image_loader(f\"/content/{style_seeds[i]}.png\")\n",
        "  content_img = image_loader(f\"/content/{content_seeds[i]}.png\")\n",
        "  output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
        "                            content_img, style_img, input_img,content_seeds[i],a[i])\n",
        "  plt.figure()\n",
        "  imshow(style_img, title='Style Image')\n",
        "  plt.figure()\n",
        "  imshow(content_img, title='Content Image')\n",
        "  imshow(output,title='mix')\n",
        "  plt.ioff()\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nni4_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "04CTldUMqRZS",
        "BgyzFj0mt7o_",
        "arfg7_aHqcT5",
        "RvZaqeaEqj7R",
        "n9lZjVVQqrPf",
        "0saduHS2q-U4",
        "MSmIESM7qUXN",
        "oPOmR6iSrUfu"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}